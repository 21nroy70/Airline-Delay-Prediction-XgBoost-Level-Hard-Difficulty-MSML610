{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1137e81d",
      "metadata": {},
      "source": [
        "### Setting up proper environment... make sure you have ran 00_colab_setup.ipynb first so you are in the same conda enviornment as me and you got the requirements.txt installed with proper versions and packages installed:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "303e75f5",
      "metadata": {},
      "source": [
        "### Also, on the top right of VS Code, make sure to changethe kernel/environment to: 'airline-delay-prediction'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "526bc5b6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/nikhilroy/Documents/MSML610/repo\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "def to_repo_root(start=Path.cwd()):\n",
        "    for p in [start, *start.parents]:\n",
        "        if (p/\"src\").exists() and (p/\"requirements.txt\").exists():\n",
        "            os.chdir(p); print(\"Project root:\", p); return\n",
        "    raise SystemExit(\"Could not locate project root (needs ./src and ./requirements.txt)\")\n",
        "to_repo_root()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e8956112",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removing: /Users/nikhilroy/Documents/MSML610/repo/data/processed/flights_enriched\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path, PurePosixPath\n",
        "import shutil\n",
        "\n",
        "enriched_dir = Path(\"data/processed/flights_enriched\")\n",
        "print(\"Removing:\", enriched_dir.resolve())\n",
        "shutil.rmtree(enriched_dir, ignore_errors=True)\n",
        "(enriched_dir).mkdir(parents=True, exist_ok=True)  # recreate empty dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "78b00ec2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote: data/processed/flights_enriched\n",
            "\n",
            "25/11/13 16:17:53 WARN Utils: Your hostname, Nikhils-MacBook-Pro-9.local resolves to a loopback address: 127.0.0.1; using 192.168.0.22 instead (on interface en0)\n",
            "25/11/13 16:17:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/11/13 16:17:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/11/13 16:18:04 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "25/11/13 16:18:08 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
            "Scaling row group sizes to 95.00% for 8 writers\n",
            "                                                                                \n"
          ]
        }
      ],
      "source": [
        "!conda run -n airline-delay-prediction python src/spark_etl.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aab6182",
      "metadata": {},
      "source": [
        "Ok cool, now that you ran it, you should see in the very top line of the output above being: Wrote: data/processed/flights_enriched\n",
        "\n",
        "This is where the processed data is stored - as parquet files because I wanted to advance this project and turn it into spark files\n",
        "so I can use Spark to read in the data because it is faster, cleaner, and is able to handle much larger volumes of data as opposed to Pandas - this is good practice for me to research Spark and gives me the oppurtunity to understand Spark and use it in a real project application."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "007c1c47",
      "metadata": {},
      "source": [
        "### Sanity check (post-ETL):\n",
        "\n",
        "I want to confirm three things after the ETL script runs:\n",
        "\n",
        "1. Spark actually wrote data/processed/flights_enriched/ and the row count matches the raw flights.csv (i.e., my joins didn’t duplicate or drop rows).\n",
        "\n",
        "2. The schema has the columns I expect (keys, coords, dates, and the is_delayed label).\n",
        "\n",
        "3. The delayed-flight rate (BTS rule: ARRIVAL_DELAY ≥ 15) looks reasonable, and there aren’t obvious null issues in the critical columns. If these checks pass, I’m confident the ETL stage is behaving as intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "42c228dd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num parts: 8\n",
            "['part-00000-b537f10e-86cd-4237-b433-81146249a4ce-c000.snappy.parquet', 'part-00001-b537f10e-86cd-4237-b433-81146249a4ce-c000.snappy.parquet', 'part-00002-b537f10e-86cd-4237-b433-81146249a4ce-c000.snappy.parquet', 'part-00003-b537f10e-86cd-4237-b433-81146249a4ce-c000.snappy.parquet', 'part-00004-b537f10e-86cd-4237-b433-81146249a4ce-c000.snappy.parquet', 'part-00005-b537f10e-86cd-4237-b433-81146249a4ce-c000.snappy.parquet', 'part-00006-b537f10e-86cd-4237-b433-81146249a4ce-c000.snappy.parquet', 'part-00007-b537f10e-86cd-4237-b433-81146249a4ce-c000.snappy.parquet']\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "parts = sorted(Path(\"data/processed/flights_enriched\").glob(\"part-*.parquet\"))\n",
        "print(\"Num parts:\", len(parts))\n",
        "print([p.name for p in parts[:8]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1ad0a345",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Git hooks.\n",
            "Git LFS initialized.\n"
          ]
        }
      ],
      "source": [
        "! git lfs install\n",
        "! git lfs pull\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3022c21a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/11/13 16:19:21 WARN Utils: Your hostname, Nikhils-MacBook-Pro-9.local resolves to a loopback address: 127.0.0.1; using 192.168.0.22 instead (on interface en0)\n",
            "25/11/13 16:19:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/11/13 16:19:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "[Stage 1:>                                                          (0 + 8) / 8]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5819079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = (SparkSession.builder\n",
        "         .appName(\"sanity\")\n",
        "         .config(\"spark.sql.parquet.enableVectorizedReader\",\"false\")\n",
        "         .config(\"spark.sql.parquet.mergeSchema\",\"false\")\n",
        "         .getOrCreate())\n",
        "df = spark.read.parquet(\"/Users/nikhilroy/Documents/MSML610/repo/data/processed/flights_enriched\")\n",
        "print(df.count())\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0a481f2c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ETL Output Exists / Counts ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enriched rows: 5,819,079\n",
            "Raw flights rows: 5,819,079\n",
            "Row delta (enriched - raw): +0  <-- should be 0 in a clean, row-preserving join\n",
            "\n",
            "=== Schema (expect keys, coords, dates, label) ===\n",
            "root\n",
            " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
            " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
            " |-- AIRLINE: string (nullable = true)\n",
            " |-- YEAR: integer (nullable = true)\n",
            " |-- MONTH: integer (nullable = true)\n",
            " |-- DAY: integer (nullable = true)\n",
            " |-- DAY_OF_WEEK: string (nullable = true)\n",
            " |-- FLIGHT_NUMBER: string (nullable = true)\n",
            " |-- TAIL_NUMBER: string (nullable = true)\n",
            " |-- SCHEDULED_DEPARTURE: string (nullable = true)\n",
            " |-- DEPARTURE_TIME: string (nullable = true)\n",
            " |-- DEPARTURE_DELAY: double (nullable = true)\n",
            " |-- TAXI_OUT: string (nullable = true)\n",
            " |-- WHEELS_OFF: string (nullable = true)\n",
            " |-- SCHEDULED_TIME: string (nullable = true)\n",
            " |-- ELAPSED_TIME: string (nullable = true)\n",
            " |-- AIR_TIME: double (nullable = true)\n",
            " |-- DISTANCE: double (nullable = true)\n",
            " |-- WHEELS_ON: string (nullable = true)\n",
            " |-- TAXI_IN: string (nullable = true)\n",
            " |-- SCHEDULED_ARRIVAL: string (nullable = true)\n",
            " |-- ARRIVAL_TIME: string (nullable = true)\n",
            " |-- ARRIVAL_DELAY: double (nullable = true)\n",
            " |-- DIVERTED: double (nullable = true)\n",
            " |-- CANCELLED: double (nullable = true)\n",
            " |-- CANCELLATION_REASON: string (nullable = true)\n",
            " |-- AIR_SYSTEM_DELAY: string (nullable = true)\n",
            " |-- SECURITY_DELAY: string (nullable = true)\n",
            " |-- AIRLINE_DELAY: string (nullable = true)\n",
            " |-- LATE_AIRCRAFT_DELAY: string (nullable = true)\n",
            " |-- WEATHER_DELAY: string (nullable = true)\n",
            " |-- FL_DATE: date (nullable = true)\n",
            " |-- dep_hour_rounded: timestamp (nullable = true)\n",
            " |-- is_delayed: integer (nullable = true)\n",
            " |-- AIRLINE_NAME: string (nullable = true)\n",
            " |-- ORIGIN_CITY: string (nullable = true)\n",
            " |-- ORIGIN_STATE: string (nullable = true)\n",
            " |-- ORIGIN_LAT: double (nullable = true)\n",
            " |-- ORIGIN_LON: double (nullable = true)\n",
            " |-- DEST_CITY: string (nullable = true)\n",
            " |-- DEST_STATE: string (nullable = true)\n",
            " |-- DEST_LAT: double (nullable = true)\n",
            " |-- DEST_LON: double (nullable = true)\n",
            "\n",
            "\n",
            "=== Preview ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-----+---+----------+-------+----------------------+--------------+-------------------+----------+----------+--------+----------+-------------+----------+\n",
            "|YEAR|MONTH|DAY|FL_DATE   |AIRLINE|AIRLINE_NAME          |ORIGIN_AIRPORT|DESTINATION_AIRPORT|ORIGIN_LAT|ORIGIN_LON|DEST_LAT|DEST_LON  |ARRIVAL_DELAY|is_delayed|\n",
            "+----+-----+---+----------+-------+----------------------+--------------+-------------------+----------+----------+--------+----------+-------------+----------+\n",
            "|2015|1    |1  |2015-01-01|AS     |Alaska Airlines Inc.  |ANC           |SEA                |61.17432  |-149.99619|47.44898|-122.30931|-22.0        |0         |\n",
            "|2015|1    |1  |2015-01-01|AA     |American Airlines Inc.|LAX           |PBI                |33.94254  |-118.40807|26.68316|-80.09559 |-9.0         |0         |\n",
            "|2015|1    |1  |2015-01-01|US     |US Airways Inc.       |SFO           |CLT                |37.619    |-122.37484|35.21401|-80.94313 |5.0          |0         |\n",
            "|2015|1    |1  |2015-01-01|AA     |American Airlines Inc.|LAX           |MIA                |33.94254  |-118.40807|25.79325|-80.29056 |-9.0         |0         |\n",
            "|2015|1    |1  |2015-01-01|AS     |Alaska Airlines Inc.  |SEA           |ANC                |47.44898  |-122.30931|61.17432|-149.99619|-21.0        |0         |\n",
            "|2015|1    |1  |2015-01-01|DL     |Delta Air Lines Inc.  |SFO           |MSP                |37.619    |-122.37484|44.88055|-93.21692 |8.0          |0         |\n",
            "|2015|1    |1  |2015-01-01|NK     |Spirit Air Lines      |LAS           |MSP                |36.08036  |-115.15233|44.88055|-93.21692 |-17.0        |0         |\n",
            "|2015|1    |1  |2015-01-01|US     |US Airways Inc.       |LAX           |CLT                |33.94254  |-118.40807|35.21401|-80.94313 |-10.0        |0         |\n",
            "|2015|1    |1  |2015-01-01|AA     |American Airlines Inc.|SFO           |DFW                |37.619    |-122.37484|32.89595|-97.0372  |-13.0        |0         |\n",
            "|2015|1    |1  |2015-01-01|DL     |Delta Air Lines Inc.  |LAS           |ATL                |36.08036  |-115.15233|33.64044|-84.42694 |-15.0        |0         |\n",
            "+----+-----+---+----------+-------+----------------------+--------------+-------------------+----------+----------+--------+----------+-------------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "=== Label Sanity ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------+\n",
            "|is_delayed|  count|\n",
            "+----------+-------+\n",
            "|      NULL| 105071|\n",
            "|         0|4650569|\n",
            "|         1|1063439|\n",
            "+----------+-------+\n",
            "\n",
            "Delayed rate: 0.186\n",
            "\n",
            "=== Null Coverage on Critical Columns ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    AIRLINE_non_null: 1.000\n",
            "AIRLINE_NAME_non_null: 1.000\n",
            "ORIGIN_AIRPORT_non_null: 1.000\n",
            "DESTINATION_AIRPORT_non_null: 1.000\n",
            " ORIGIN_LAT_non_null: 0.916\n",
            " ORIGIN_LON_non_null: 0.916\n",
            "   DEST_LAT_non_null: 0.916\n",
            "   DEST_LON_non_null: 0.916\n",
            "ARRIVAL_DELAY_non_null: 0.982\n",
            "    FL_DATE_non_null: 1.000\n",
            " is_delayed_non_null: 0.982\n",
            "\n",
            "=== Summary ===\n",
            "ETL looks good: row count matches, schema sane, label distribution reasonable, and nulls minimal.\n"
          ]
        }
      ],
      "source": [
        "# Runs the sanity script inside the conda env so Spark starts cleanly\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"etl_sanity_check\").getOrCreate()\n",
        "\n",
        "enriched_path = \"data/processed/flights_enriched\"\n",
        "df = spark.read.parquet(enriched_path)\n",
        "raw = spark.read.option(\"header\", True).csv(\"data/raw/flights.csv\")\n",
        "\n",
        "print(\"=== ETL Output Exists / Counts ===\")\n",
        "out_rows = df.count()\n",
        "raw_rows = raw.count()\n",
        "diff = out_rows - raw_rows\n",
        "print(f\"Enriched rows: {out_rows:,}\")\n",
        "print(f\"Raw flights rows: {raw_rows:,}\")\n",
        "print(f\"Row delta (enriched - raw): {diff:+,}  <-- should be 0 in a clean, row-preserving join\")\n",
        "\n",
        "print(\"\\n=== Schema (expect keys, coords, dates, label) ===\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"\\n=== Preview ===\")\n",
        "df.select(\n",
        "    \"YEAR\",\"MONTH\",\"DAY\",\"FL_DATE\",\n",
        "    \"AIRLINE\",\"AIRLINE_NAME\",\n",
        "    \"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
        "    \"ORIGIN_LAT\",\"ORIGIN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
        "    \"ARRIVAL_DELAY\",\"is_delayed\"\n",
        ").show(10, truncate=False)\n",
        "\n",
        "print(\"\\n=== Label Sanity ===\")\n",
        "delay_rate = df.select(F.mean(F.col(\"is_delayed\").cast(\"double\")).alias(\"delay_rate\")).first()[\"delay_rate\"]\n",
        "df.groupBy(\"is_delayed\").count().orderBy(\"is_delayed\").show()\n",
        "print(f\"Delayed rate: {delay_rate:.3f}\")\n",
        "\n",
        "print(\"\\n=== Null Coverage on Critical Columns ===\")\n",
        "critical = [\n",
        "    \"AIRLINE\",\"AIRLINE_NAME\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
        "    \"ORIGIN_LAT\",\"ORIGIN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
        "    \"ARRIVAL_DELAY\",\"FL_DATE\",\"is_delayed\"\n",
        "]\n",
        "cover = df.select([\n",
        "    (1 - (F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)) / F.count(F.lit(1)))).alias(f\"{c}_non_null\")\n",
        "    for c in critical\n",
        "]).first().asDict()\n",
        "for k, v in cover.items():\n",
        "    print(f\"{k:>20}: {v:.3f}\")\n",
        "\n",
        "issues = []\n",
        "if diff != 0:\n",
        "    issues.append(f\"Row mismatch: enriched={out_rows} vs raw={raw_rows} (delta {diff:+})\")\n",
        "for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"ARRIVAL_DELAY\",\"is_delayed\",\"FL_DATE\"]:\n",
        "    if cover.get(f\"{c}_non_null\", 0) < 0.95:\n",
        "        issues.append(f\"High nulls in {c} (non-null rate={cover.get(f'{c}_non_null', 0):.3f})\")\n",
        "\n",
        "print(\"\\n=== Summary ===\")\n",
        "if issues:\n",
        "    print(\"Potential issues detected:\")\n",
        "    for i in issues:\n",
        "        print(\" -\", i)\n",
        "else:\n",
        "    print(\"ETL looks good: row count matches, schema sane, label distribution reasonable, and nulls minimal.\")\n",
        "\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85815c21",
      "metadata": {},
      "source": [
        "Just a quick note while we are here (or if you don't wanna run the above code/see the output for my sanity check):\n",
        "\n",
        "18.6% of our data has a delayed flight - so we know we have an imbalance of our y classes, so accuracy is an invalid score here!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89d2aa4f",
      "metadata": {},
      "source": [
        "### Ok, now running the 2nd Python files below (which actually uses weather_meteostat.py as a helper file to run merge_weather.py)\n",
        "\n",
        "### Beaware, this takes time to run because I had to do extensive research to find an applicable dataset that would appropiately be compatbile with the current dataset which was provided since there was no info from weather data as provided in the data given from instructions. Therefore, I had to research and find a suitable data to be able to obtain weather data which would fit the current data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote merged dataset: data/processed/flights_with_weather.parquet, shape=(5819079, 54)\n",
            "\n",
            "Warning: Cannot load hourly/2014/8RXA6.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/8RXA6.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/8RXA6.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2014/KC290.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/KC290.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/KC290.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2014/KLDJ0.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/KLDJ0.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/KLDJ0.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2014/OQRHP.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/OQRHP.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/OQRHP.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2014/OZNF7.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/OZNF7.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/OZNF7.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2014/56FNV.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/56FNV.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/56FNV.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2014/SC9N0.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/SC9N0.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/SC9N0.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2014/INGE8.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/INGE8.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/INGE8.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2014/M44D9.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/M44D9.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/M44D9.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2014/LJ957.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/LJ957.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/LJ957.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2014/KBTR0.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/KBTR0.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/KBTR0.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2014/A4QYJ.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/A4QYJ.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/A4QYJ.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2014/O26VE.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/O26VE.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/O26VE.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2014/KBEC0.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/KBEC0.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/KBEC0.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2014/GDDIA.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/GDDIA.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/GDDIA.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2014/O5PFW.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/O5PFW.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/O5PFW.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2014/91275.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/91275.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/91275.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2014/KN030.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2015/KN030.csv.gz from https://bulk.meteostat.net/v2/\n",
            "Warning: Cannot load hourly/2016/KN030.csv.gz from https://bulk.meteostat.net/v2/\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Weather join (Meteostat)\n",
        "\n",
        "!conda run -n airline-delay-prediction python src/merge_weather.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e238b29",
      "metadata": {},
      "source": [
        "Ok cool, now that you ran it, you should see in the very last line of the output above being: Wrote: data/processed/flights_with_weather.parquet\n",
        "\n",
        "This is where the processed data is stored - as parquet files because I wanted to advance this project and turn it into spark files\n",
        "so I can use Spark to read in the data because it is faster, cleaner, and is able to handle much larger volumes of data as opposed to Pandas - this is good practice for me to research Spark and gives me the oppurtunity to understand Spark and use it in a real project application."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bd0f3b9",
      "metadata": {},
      "source": [
        "### Doing a quick sanity check for merge_weather.py:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9af6ba44",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows: 5819079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+------------------+------------------+------------------+------------------+\n",
            "|station_id_non_null|     temp_non_null|     rhum_non_null|     prcp_non_null|     wspd_non_null|\n",
            "+-------------------+------------------+------------------+------------------+------------------+\n",
            "| 0.9156619114468114|0.8449048380336476|0.8447427848977476|0.7838001511923107|0.8440383778945088|\n",
            "+-------------------+------------------+------------------+------------------+------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+----------+-------------------+----+-----+----+----+------+----------+\n",
            "|ORIGIN_AIRPORT|station_id|dep_hour_rounded   |temp|rhum |prcp|wspd|pres  |is_delayed|\n",
            "+--------------+----------+-------------------+----+-----+----+----+------+----------+\n",
            "|RSW           |KRSW0     |2015-07-26 16:00:00|23.3|100.0|71.9|33.5|1014.2|0.0       |\n",
            "|RSW           |KRSW0     |2015-07-26 16:00:00|23.3|100.0|71.9|33.5|1014.2|0.0       |\n",
            "|RSW           |KRSW0     |2015-07-26 16:00:00|23.3|100.0|71.9|33.5|1014.2|1.0       |\n",
            "|RSW           |KRSW0     |2015-07-26 16:00:00|23.3|100.0|71.9|33.5|1014.2|1.0       |\n",
            "|RSW           |KRSW0     |2015-07-26 16:00:00|23.3|100.0|71.9|33.5|1014.2|1.0       |\n",
            "|CHS           |72208     |2015-08-31 11:00:00|23.9|94.0 |71.4|0.0 |1016.4|0.0       |\n",
            "|CHS           |72208     |2015-08-31 11:00:00|23.9|94.0 |71.4|0.0 |1016.4|1.0       |\n",
            "|CHS           |72208     |2015-08-31 11:00:00|23.9|94.0 |71.4|0.0 |1016.4|0.0       |\n",
            "|CHS           |72208     |2015-08-31 11:00:00|23.9|94.0 |71.4|0.0 |1016.4|0.0       |\n",
            "|MOB           |72223     |2015-09-27 23:00:00|22.8|100.0|68.1|22.3|1012.9|1.0       |\n",
            "+--------------+----------+-------------------+----+-----+----+----+------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession, functions as F\n",
        "spark = SparkSession.builder.appName(\"wx_sanity\").getOrCreate()\n",
        "wx = spark.read.parquet(\"data/processed/flights_with_weather.parquet\")\n",
        "\n",
        "print(\"Rows:\", wx.count())\n",
        "wx.select(\n",
        "    (1 - F.avg(F.when(F.col(\"station_id\").isNull(), 1).otherwise(0))).alias(\"station_id_non_null\"),\n",
        "    (1 - F.avg(F.when(F.col(\"temp\").isNull(), 1).otherwise(0))).alias(\"temp_non_null\"),\n",
        "    (1 - F.avg(F.when(F.col(\"rhum\").isNull(), 1).otherwise(0))).alias(\"rhum_non_null\"),\n",
        "    (1 - F.avg(F.when(F.col(\"prcp\").isNull(), 1).otherwise(0))).alias(\"prcp_non_null\"),\n",
        "    (1 - F.avg(F.when(F.col(\"wspd\").isNull(), 1).otherwise(0))).alias(\"wspd_non_null\")\n",
        ").show()\n",
        "\n",
        "wx.select(\"ORIGIN_AIRPORT\",\"station_id\",\"dep_hour_rounded\",\"temp\",\"rhum\",\"prcp\",\"wspd\",\"pres\",\"is_delayed\")\\\n",
        "  .orderBy(F.desc(\"prcp\")).show(10, truncate=False)\n",
        "\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5e35131",
      "metadata": {},
      "source": [
        "### Weather sanity: \n",
        "\n",
        "Mapping origin → nearest Meteostat station worked for ~92% of flights. Most core wx fields (temp, rhum, wspd) exist ~84–85% of the time; prcp is lower (~78%) but still OK. This is good enough to keep weather in the feature set. Multiple flights can share the exact same hourly weather (same station + hour), so seeing duplicates in the preview is expected."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09a9ce38",
      "metadata": {},
      "source": [
        "## Final Sanity Check Before Moving on To EDA + Modeling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "81c39c34",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ALL COLUMNS (54 total) ===\n",
            "01. DESTINATION_AIRPORT  (object)\n",
            "02. ORIGIN_AIRPORT  (object)\n",
            "03. AIRLINE  (object)\n",
            "04. YEAR  (int32)\n",
            "05. MONTH  (int32)\n",
            "06. DAY  (int32)\n",
            "07. DAY_OF_WEEK  (object)\n",
            "08. FLIGHT_NUMBER  (object)\n",
            "09. TAIL_NUMBER  (object)\n",
            "10. SCHEDULED_DEPARTURE  (object)\n",
            "11. DEPARTURE_TIME  (object)\n",
            "12. DEPARTURE_DELAY  (float64)\n",
            "13. TAXI_OUT  (object)\n",
            "14. WHEELS_OFF  (object)\n",
            "15. SCHEDULED_TIME  (object)\n",
            "16. ELAPSED_TIME  (object)\n",
            "17. AIR_TIME  (float64)\n",
            "18. DISTANCE  (float64)\n",
            "19. WHEELS_ON  (object)\n",
            "20. TAXI_IN  (object)\n",
            "21. SCHEDULED_ARRIVAL  (object)\n",
            "22. ARRIVAL_TIME  (object)\n",
            "23. ARRIVAL_DELAY  (float64)\n",
            "24. DIVERTED  (float64)\n",
            "25. CANCELLED  (float64)\n",
            "26. CANCELLATION_REASON  (object)\n",
            "27. AIR_SYSTEM_DELAY  (object)\n",
            "28. SECURITY_DELAY  (object)\n",
            "29. AIRLINE_DELAY  (object)\n",
            "30. LATE_AIRCRAFT_DELAY  (object)\n",
            "31. WEATHER_DELAY  (object)\n",
            "32. FL_DATE  (object)\n",
            "33. dep_hour_rounded  (datetime64[us])\n",
            "34. is_delayed  (int64)\n",
            "35. AIRLINE_NAME  (object)\n",
            "36. ORIGIN_CITY  (object)\n",
            "37. ORIGIN_STATE  (object)\n",
            "38. ORIGIN_LAT  (float64)\n",
            "39. ORIGIN_LON  (float64)\n",
            "40. DEST_CITY  (object)\n",
            "41. DEST_STATE  (object)\n",
            "42. DEST_LAT  (float64)\n",
            "43. DEST_LON  (float64)\n",
            "44. station_id  (object)\n",
            "45. temp  (float64)\n",
            "46. dwpt  (float64)\n",
            "47. rhum  (float64)\n",
            "48. prcp  (float64)\n",
            "49. snow  (float64)\n",
            "50. wdir  (float64)\n",
            "51. wspd  (float64)\n",
            "52. wpgt  (float64)\n",
            "53. pres  (float64)\n",
            "54. tsun  (float64)\n",
            "\n",
            "=== QUICK SUMMARY ===\n",
            "Total rows: 5,819,079\n",
            "Unique airlines: 14\n",
            "Unique origin airports: 322\n",
            "Unique destination airports: 322\n",
            "Delayed flights (is_delayed=1): 1,101,634\n",
            "On-time flights (is_delayed=0): 4,717,445\n",
            "\n",
            " SHOWING TOP 10 ROWS WITH ALL COLUMNS AN EXAMPLE FOR A FEEL OF FINAL DATASET MERGED WITH WEATHER\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DESTINATION_AIRPORT</th>\n",
              "      <th>ORIGIN_AIRPORT</th>\n",
              "      <th>AIRLINE</th>\n",
              "      <th>YEAR</th>\n",
              "      <th>MONTH</th>\n",
              "      <th>DAY</th>\n",
              "      <th>DAY_OF_WEEK</th>\n",
              "      <th>FLIGHT_NUMBER</th>\n",
              "      <th>TAIL_NUMBER</th>\n",
              "      <th>SCHEDULED_DEPARTURE</th>\n",
              "      <th>...</th>\n",
              "      <th>temp</th>\n",
              "      <th>dwpt</th>\n",
              "      <th>rhum</th>\n",
              "      <th>prcp</th>\n",
              "      <th>snow</th>\n",
              "      <th>wdir</th>\n",
              "      <th>wspd</th>\n",
              "      <th>wpgt</th>\n",
              "      <th>pres</th>\n",
              "      <th>tsun</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SEA</td>\n",
              "      <td>ANC</td>\n",
              "      <td>AS</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>98</td>\n",
              "      <td>N407AS</td>\n",
              "      <td>0005</td>\n",
              "      <td>...</td>\n",
              "      <td>3.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>82.0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>11.2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1024.8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PBI</td>\n",
              "      <td>LAX</td>\n",
              "      <td>AA</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2336</td>\n",
              "      <td>N3KUAA</td>\n",
              "      <td>0010</td>\n",
              "      <td>...</td>\n",
              "      <td>9.4</td>\n",
              "      <td>-10.2</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1018.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CLT</td>\n",
              "      <td>SFO</td>\n",
              "      <td>US</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>840</td>\n",
              "      <td>N171US</td>\n",
              "      <td>0020</td>\n",
              "      <td>...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>61.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1018.5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MIA</td>\n",
              "      <td>LAX</td>\n",
              "      <td>AA</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>258</td>\n",
              "      <td>N3HYAA</td>\n",
              "      <td>0020</td>\n",
              "      <td>...</td>\n",
              "      <td>9.4</td>\n",
              "      <td>-10.2</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1018.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ANC</td>\n",
              "      <td>SEA</td>\n",
              "      <td>AS</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>135</td>\n",
              "      <td>N527AS</td>\n",
              "      <td>0025</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.6</td>\n",
              "      <td>66.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.0</td>\n",
              "      <td>14.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1032.2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>MSP</td>\n",
              "      <td>SFO</td>\n",
              "      <td>DL</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>806</td>\n",
              "      <td>N3730B</td>\n",
              "      <td>0025</td>\n",
              "      <td>...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>61.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1018.5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>MSP</td>\n",
              "      <td>LAS</td>\n",
              "      <td>NK</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>612</td>\n",
              "      <td>N635NK</td>\n",
              "      <td>0025</td>\n",
              "      <td>...</td>\n",
              "      <td>2.8</td>\n",
              "      <td>-8.9</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>170.0</td>\n",
              "      <td>9.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1015.4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>CLT</td>\n",
              "      <td>LAX</td>\n",
              "      <td>US</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2013</td>\n",
              "      <td>N584UW</td>\n",
              "      <td>0030</td>\n",
              "      <td>...</td>\n",
              "      <td>9.4</td>\n",
              "      <td>-10.2</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1018.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>DFW</td>\n",
              "      <td>SFO</td>\n",
              "      <td>AA</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1112</td>\n",
              "      <td>N3LAAA</td>\n",
              "      <td>0030</td>\n",
              "      <td>...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>61.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1018.5</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ATL</td>\n",
              "      <td>LAS</td>\n",
              "      <td>DL</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1173</td>\n",
              "      <td>N826DN</td>\n",
              "      <td>0030</td>\n",
              "      <td>...</td>\n",
              "      <td>2.8</td>\n",
              "      <td>-8.9</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>170.0</td>\n",
              "      <td>9.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1015.4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 54 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  DESTINATION_AIRPORT ORIGIN_AIRPORT AIRLINE  YEAR  MONTH  DAY DAY_OF_WEEK  \\\n",
              "0                 SEA            ANC      AS  2015      1    1           4   \n",
              "1                 PBI            LAX      AA  2015      1    1           4   \n",
              "2                 CLT            SFO      US  2015      1    1           4   \n",
              "3                 MIA            LAX      AA  2015      1    1           4   \n",
              "4                 ANC            SEA      AS  2015      1    1           4   \n",
              "5                 MSP            SFO      DL  2015      1    1           4   \n",
              "6                 MSP            LAS      NK  2015      1    1           4   \n",
              "7                 CLT            LAX      US  2015      1    1           4   \n",
              "8                 DFW            SFO      AA  2015      1    1           4   \n",
              "9                 ATL            LAS      DL  2015      1    1           4   \n",
              "\n",
              "  FLIGHT_NUMBER TAIL_NUMBER SCHEDULED_DEPARTURE  ...  temp  dwpt  rhum prcp  \\\n",
              "0            98      N407AS                0005  ...   3.3   0.5  82.0  1.3   \n",
              "1          2336      N3KUAA                0010  ...   9.4 -10.2  24.0  0.0   \n",
              "2           840      N171US                0020  ...  10.0   2.8  61.0  0.0   \n",
              "3           258      N3HYAA                0020  ...   9.4 -10.2  24.0  0.0   \n",
              "4           135      N527AS                0025  ...   0.0  -5.6  66.0  0.0   \n",
              "5           806      N3730B                0025  ...  10.0   2.8  61.0  0.0   \n",
              "6           612      N635NK                0025  ...   2.8  -8.9  42.0  0.0   \n",
              "7          2013      N584UW                0030  ...   9.4 -10.2  24.0  0.0   \n",
              "8          1112      N3LAAA                0030  ...  10.0   2.8  61.0  0.0   \n",
              "9          1173      N826DN                0030  ...   2.8  -8.9  42.0  0.0   \n",
              "\n",
              "  snow   wdir  wspd  wpgt    pres tsun  \n",
              "0  NaN  150.0  11.2   NaN  1024.8  NaN  \n",
              "1  NaN    NaN   0.0   NaN  1018.0  NaN  \n",
              "2  NaN    NaN   0.0   NaN  1018.5  NaN  \n",
              "3  NaN    NaN   0.0   NaN  1018.0  NaN  \n",
              "4  NaN   30.0  14.8   NaN  1032.2  NaN  \n",
              "5  NaN    NaN   0.0   NaN  1018.5  NaN  \n",
              "6  NaN  170.0   9.4   NaN  1015.4  NaN  \n",
              "7  NaN    NaN   0.0   NaN  1018.0  NaN  \n",
              "8  NaN    NaN   0.0   NaN  1018.5  NaN  \n",
              "9  NaN  170.0   9.4   NaN  1015.4  NaN  \n",
              "\n",
              "[10 rows x 54 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pyarrow.dataset as ds\n",
        "\n",
        "# Load entire merged dataset into memory (EDA-level inspection)\n",
        "dataset = ds.dataset(\"data/processed/flights_with_weather.parquet\", format=\"parquet\")\n",
        "df = dataset.to_table().to_pandas()\n",
        "\n",
        "# Fix numeric + label columns\n",
        "numeric_cols = [\n",
        "    \"DEPARTURE_DELAY\", \"ARRIVAL_DELAY\", \"DISTANCE\", \"AIR_TIME\",\n",
        "    \"temp\", \"rhum\", \"prcp\", \"wspd\", \"pres\"\n",
        "]\n",
        "for c in numeric_cols:\n",
        "    if c in df.columns:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "if \"is_delayed\" in df.columns:\n",
        "    df[\"is_delayed\"] = df[\"is_delayed\"].fillna(0).astype(int)\n",
        "\n",
        "# --- Print column names + data types ---\n",
        "print(\"=== ALL COLUMNS ({} total) ===\".format(len(df.columns)))\n",
        "for i, col in enumerate(df.columns, start=1):\n",
        "    print(f\"{i:02d}. {col}  ({df[col].dtype})\")\n",
        "\n",
        "print(\"\\n=== QUICK SUMMARY ===\")\n",
        "print(f\"Total rows: {len(df):,}\")\n",
        "print(f\"Unique airlines: {df['AIRLINE_NAME'].nunique() if 'AIRLINE_NAME' in df.columns else 'N/A'}\")\n",
        "print(f\"Unique origin airports: {df['ORIGIN_AIRPORT'].nunique() if 'ORIGIN_AIRPORT' in df.columns else 'N/A'}\")\n",
        "print(f\"Unique destination airports: {df['DESTINATION_AIRPORT'].nunique() if 'DESTINATION_AIRPORT' in df.columns else 'N/A'}\")\n",
        "print(f\"Delayed flights (is_delayed=1): {df['is_delayed'].sum():,}\")\n",
        "print(f\"On-time flights (is_delayed=0): {(df['is_delayed']==0).sum():,}\")\n",
        "\n",
        "print(\"\\n SHOWING TOP 10 ROWS WITH ALL COLUMNS AN EXAMPLE FOR A FEEL OF FINAL DATASET MERGED WITH WEATHER\")\n",
        "\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e413dae",
      "metadata": {},
      "source": [
        "Units: we used SI (Celsius, mm, m/s)"
      ]
    }
  ],
  "metadata": {
    "colab": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
