{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820ec773",
   "metadata": {},
   "source": [
    "# 04 — Tuning Models (Time-aware CV)\n",
    "\n",
    "This notebook runs the XGBoost tuner that performs chronological cross-validation **inside the training window**, selects by **AUC-PR**, and then trains a final model and evaluates on the held-out validation window.\n",
    "\n",
    "Make sure to change your kernel on the upper right (on vs-code) to the airline-delay-prediction (Python 3.10)\n",
    "\n",
    "### Conda / environment\n",
    "```bash\n",
    "# create (or activate) your env with xgboost, pyarrow, sklearn, matplotlib\n",
    "conda create -n airline-delay-prediction python=3.10 -y\n",
    "conda activate airline-delay-prediction\n",
    "pip install xgboost==2.0.3 pyarrow pandas scikit-learn matplotlib joblib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9989cd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/nikhilroy/Documents/MSML610/repo\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def to_repo_root(start=Path.cwd()):\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p/\"src\").exists() and (p/\"requirements.txt\").exists():\n",
    "            os.chdir(p); print(\"Project root:\", p); return\n",
    "    raise SystemExit(\"Could not locate project root (needs ./src and ./requirements.txt)\")\n",
    "\n",
    "to_repo_root()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3bc5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils_model import (\n",
    "    SCHEMA, BASE_CATEGORICAL, BASE_NUMERIC,\n",
    "    load_model, load_metrics, predict_proba, coerce_schema,\n",
    "    pick_threshold, load_all_metrics_table, score_row, score_dataframe\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8f9108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found libomp candidates: ['/Users/nikhilroy/opt/anaconda3/envs/airline-delay-prediction/lib/libomp.dylib']\n",
      "XGBoost version: 2.1.1\n",
      "XGBoost OK\n"
     ]
    }
   ],
   "source": [
    "import os, sys, glob, ctypes, importlib\n",
    "\n",
    "env_prefix = os.environ.get(\"CONDA_PREFIX\", sys.prefix)\n",
    "candidates = glob.glob(os.path.join(env_prefix, \"lib\", \"libomp*.dylib\"))\n",
    "print(\"Found libomp candidates:\", candidates)\n",
    "\n",
    "if not candidates:\n",
    "    raise RuntimeError(\"libomp.dylib not found in the conda env. Make sure llvm-openmp installed in THIS env.\")\n",
    "\n",
    "libomp_path = candidates[0]\n",
    "# Preload OpenMP before importing xgboost\n",
    "ctypes.CDLL(libomp_path)\n",
    "\n",
    "# (Optional) nudge the loader to see the env's lib directory first\n",
    "os.environ[\"DYLD_LIBRARY_PATH\"] = env_prefix + \"/lib:\" + os.environ.get(\"DYLD_LIBRARY_PATH\",\"\")\n",
    "\n",
    "# Now import/test xgboost\n",
    "import xgboost as xgb, numpy as np\n",
    "print(\"XGBoost version:\", xgb.__version__)\n",
    "X = np.random.randn(200, 10); y = (np.random.rand(200) > 0.8).astype(int)\n",
    "d = xgb.DMatrix(X, label=y)\n",
    "xgb.train({'objective':'binary:logistic','tree_method':'hist','verbosity':0}, d, num_boost_round=1)\n",
    "print(\"XGBoost OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3696c198",
   "metadata": {},
   "source": [
    "## Tuning our XGBoost model with our features (departure_delay included) - with cross-validation and Bayesian Optimization For Updating each factor that is varied:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e241e606",
   "metadata": {},
   "source": [
    "This took over 40 hours to run soooooooooooooooo hopefully my grade is worth this commitment (literally full work week hours just to run the tuning part of modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f056558",
   "metadata": {},
   "source": [
    "Data & I/O\n",
    "\n",
    "--in_path (str, default: data/processed/flights_with_weather.parquet)\n",
    "Path to the parquet with your engineered dataset.\n",
    "\n",
    "--out_dir (str, default: models)\n",
    "Where all artifacts (CSV of trials, model, metrics, plots) are written.\n",
    "\n",
    "--tag (str, default: tuned_all_features_bo)\n",
    "Prefix for artifact filenames so runs don’t overwrite each other.\n",
    "\n",
    "Train/validation split\n",
    "\n",
    "--split (time|random, default: time)\n",
    "time: sorts by FL_DATE, uses the last eval_size portion as final validation.\n",
    "random: random stratified split.\n",
    "\n",
    "--eval_size (float, default: 0.20)\n",
    "Fraction of data reserved for the final validation holdout.\n",
    "\n",
    "--use_departure_delay (bool-ish str, default: true)\n",
    "If true, includes DEPARTURE_DELAY as a feature. If false, excludes it.\n",
    "\n",
    "CV (for Bayesian objective)\n",
    "\n",
    "--cv_folds (int, default: 5)\n",
    "Number of time-aware folds inside the train pool. We only validate on future blocks (no leakage).\n",
    "\n",
    "Bayesian Optimization (Optuna)\n",
    "\n",
    "--bo_trials (int, default: 40)\n",
    "Total trials to sample. More trials = better search but slower.\n",
    "\n",
    "--bo_startup_trials (int, default: 10)\n",
    "Initial random/TPE warm-up trials before the sampler “gets smart.”\n",
    "\n",
    "--bo_timeout (int seconds, default: 0)\n",
    "Hard wall-clock limit for the search. 0 = no limit.\n",
    "\n",
    "Boosting / early stopping (used in CV and final fit)\n",
    "\n",
    "--n_rounds (int, default: 1200)\n",
    "Max number of boosting rounds allowed.\n",
    "\n",
    "--early_stopping (int, default: 100)\n",
    "Stop if the validation metric doesn’t improve for this many rounds. Keeps the best iteration.\n",
    "\n",
    "Hyperparameter search bounds (inclusive)\n",
    "\n",
    "These define the search space Optuna samples from. Tighten if you know the sweet spot; widen if you want more exploration.\n",
    "\n",
    "--lr_low, --lr_high (floats, default: 0.03 … 0.2)\n",
    "Learning rate (eta). Lower = slower but often more stable; higher can converge faster but overfit.\n",
    "\n",
    "--max_depth_low, --max_depth_high (ints, default: 5 … 9)\n",
    "Tree depth. Larger can fit more complex interactions; too large risks overfitting.\n",
    "\n",
    "--min_child_weight_low, --min_child_weight_high (ints, default: 1 … 8)\n",
    "Minimum sum of instance weight needed in a child. Higher = more conservative (simpler trees).\n",
    "\n",
    "--subsample_low, --subsample_high (floats, default: 0.6 … 1.0)\n",
    "Row subsampling per tree. <1.0 adds randomness, generally helps generalization.\n",
    "\n",
    "--colsample_bytree_low, --colsample_bytree_high (floats, default: 0.6 … 1.0)\n",
    "Column subsampling per tree. Similar bias-variance trade-off as subsample.\n",
    "\n",
    "--reg_alpha_low, --reg_alpha_high (floats, default: 1e-8 … 1.0, log scale)\n",
    "L1 regularization. Drives sparsity; can zero out weak splits.\n",
    "\n",
    "--reg_lambda_low, --reg_lambda_high (floats, default: 1e-2 … 10.0, log scale)\n",
    "L2 regularization. Smooths weights; often stabilizes training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9225acff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-14 23:20:50,114] A new study created in memory with name: no-name-4a8ca277-4e9f-43be-aaa6-2f7098cb077c\n",
      "[I 2025-11-15 00:32:07,274] Trial 0 finished with value: 0.9125777861991686 and parameters: {'learning_rate': 0.08757364264182173, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.8805135844073053, 'colsample_bytree': 0.6817038283811238, 'reg_alpha': 9.572006494251443e-06, 'reg_lambda': 0.09496277503603154}. Best is trial 0 with value: 0.9125777861991686.\n",
      "[I 2025-11-15 01:05:38,672] Trial 1 finished with value: 0.9108925521988819 and parameters: {'learning_rate': 0.12007457923586612, 'max_depth': 9, 'min_child_weight': 7, 'subsample': 0.6841278824951569, 'colsample_bytree': 0.7284174394919806, 'reg_alpha': 0.2390630533840136, 'reg_lambda': 0.14296883396345103}. Best is trial 0 with value: 0.9125777861991686.\n",
      "[I 2025-11-15 01:36:34,865] Trial 2 finished with value: 0.9115564443909854 and parameters: {'learning_rate': 0.1770459916789657, 'max_depth': 8, 'min_child_weight': 8, 'subsample': 0.8327929842925241, 'colsample_bytree': 0.6765562600651132, 'reg_alpha': 0.05474505790707832, 'reg_lambda': 0.7244925916013392}. Best is trial 0 with value: 0.9125777861991686.\n",
      "[I 2025-11-15 02:50:42,238] Trial 3 finished with value: 0.9119940258540756 and parameters: {'learning_rate': 0.09349627084555652, 'max_depth': 6, 'min_child_weight': 5, 'subsample': 0.7067059870446146, 'colsample_bytree': 0.7343260785198771, 'reg_alpha': 0.5182464780130581, 'reg_lambda': 0.5694281179576327}. Best is trial 0 with value: 0.9125777861991686.\n",
      "[I 2025-11-15 03:20:32,552] Trial 4 finished with value: 0.9120188878253941 and parameters: {'learning_rate': 0.14890104394937215, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.9406390239380737, 'colsample_bytree': 0.6725629023550166, 'reg_alpha': 0.004182561259940469, 'reg_lambda': 0.2554530345363232}. Best is trial 0 with value: 0.9125777861991686.\n",
      "[I 2025-11-15 04:03:09,562] Trial 5 finished with value: 0.9117011986382595 and parameters: {'learning_rate': 0.12630882904968027, 'max_depth': 7, 'min_child_weight': 7, 'subsample': 0.7424245308390716, 'colsample_bytree': 0.8214185925677334, 'reg_alpha': 4.883618328253037e-08, 'reg_lambda': 0.015254171974589233}. Best is trial 0 with value: 0.9125777861991686.\n",
      "[I 2025-11-15 05:01:24,464] Trial 6 finished with value: 0.912128256567464 and parameters: {'learning_rate': 0.12427276619156809, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.8445343597756053, 'colsample_bytree': 0.8264748040578173, 'reg_alpha': 0.13613830915757022, 'reg_lambda': 0.07285972363357082}. Best is trial 0 with value: 0.9125777861991686.\n",
      "[I 2025-11-15 06:06:39,391] Trial 7 finished with value: 0.9120996363845597 and parameters: {'learning_rate': 0.08210896194339484, 'max_depth': 7, 'min_child_weight': 3, 'subsample': 0.7598040360484408, 'colsample_bytree': 0.7624818817260208, 'reg_alpha': 0.0005758945513471427, 'reg_lambda': 0.5008242014493559}. Best is trial 0 with value: 0.9125777861991686.\n",
      "[I 2025-11-15 06:30:26,232] Trial 8 finished with value: 0.9109897077406557 and parameters: {'learning_rate': 0.19387973853624943, 'max_depth': 8, 'min_child_weight': 8, 'subsample': 0.861090207017855, 'colsample_bytree': 0.9509956486894078, 'reg_alpha': 2.745678578257814e-08, 'reg_lambda': 0.02509076261899221}. Best is trial 0 with value: 0.9125777861991686.\n",
      "[I 2025-11-15 07:38:17,295] Trial 9 finished with value: 0.9125438384589204 and parameters: {'learning_rate': 0.08537679149268669, 'max_depth': 6, 'min_child_weight': 8, 'subsample': 0.815018795749592, 'colsample_bytree': 0.7783299100583972, 'reg_alpha': 0.05983641912644928, 'reg_lambda': 0.06970874861839667}. Best is trial 0 with value: 0.9125777861991686.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best hyperparams: {'learning_rate': 0.08757364264182173, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.8805135844073053, 'colsample_bytree': 0.6817038283811238, 'reg_alpha': 9.572006494251443e-06, 'reg_lambda': 0.09496277503603154}\n",
      "\n",
      "tuned_all_features_bo: AUC=0.962  AP=0.918  F1=0.839  P=0.895  R=0.790  (best_iter=983)\n",
      "Saved: models/tuned_all_features_bo_tune_trials.csv and model/metrics/plots under models/\n"
     ]
    }
   ],
   "source": [
    "# Bayesian tuning with Optuna (maximize AP over time-aware CV)\n",
    "%run src/tuning_models.py \\\n",
    "  --in_path data/processed/flights_with_weather.parquet \\\n",
    "  --out_dir models \\\n",
    "  --split time --eval_size 0.20 \\\n",
    "  --use_departure_delay true \\\n",
    "  --tag tuned_all_features_bo \\\n",
    "  --cv_folds 5 \\\n",
    "  --bo_trials 10 \\\n",
    "  --bo_startup_trials 10 \\\n",
    "  --bo_timeout 0 \\\n",
    "  --n_rounds 1001 \\\n",
    "  --early_stopping 100 \\\n",
    "  --lr_low 0.03 \\\n",
    "  --lr_high 0.2 \\\n",
    "  --max_depth_low 5 \\\n",
    "  --max_depth_high 9 \\\n",
    "  --min_child_weight_low 1 \\\n",
    "  --min_child_weight_high 8 \\\n",
    "  --subsample_low 0.6 \\\n",
    "  --subsample_high 1.0 \\\n",
    "  --colsample_bytree_low 0.6 \\\n",
    "  --colsample_bytree_high 1.0 \\\n",
    "  --reg_alpha_low 1e-8 \\\n",
    "  --reg_alpha_high 1.0 \\\n",
    "  --reg_lambda_low 1e-2 \\\n",
    "  --reg_lambda_high 10.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bfaf10",
   "metadata": {},
   "source": [
    "As you can see from the output above (or just looking at \"models/tuned_all_features_bo_tune_trials.csv\"), you can see that despite tuning our various paremters that make up the XgBoost model using a powerful Bayesian Optimizer across several variations, that our final XgBoost model is barely increasing in performance. This is because after hundreds and hundreds of n_estimators as and our iterations keep increasing, we learn from our mistakes and built upon our trees to produce more precise and recall more accurate classes. Thus, any of these models are valid and produce very close performance (rip my 40 hours of running this though, side note). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79e494d",
   "metadata": {},
   "source": [
    "### Let's see our trial table and how it looks, sorting by descending average precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af194c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>cv_ap</th>\n",
       "      <th>cv_auc</th>\n",
       "      <th>cv_logloss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>reg_lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.912578</td>\n",
       "      <td>0.961354</td>\n",
       "      <td>0.204983</td>\n",
       "      <td>0.087574</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.880514</td>\n",
       "      <td>0.681704</td>\n",
       "      <td>9.572006e-06</td>\n",
       "      <td>0.094963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.912544</td>\n",
       "      <td>0.961229</td>\n",
       "      <td>0.203870</td>\n",
       "      <td>0.085377</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.815019</td>\n",
       "      <td>0.778330</td>\n",
       "      <td>5.983642e-02</td>\n",
       "      <td>0.069709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.912128</td>\n",
       "      <td>0.960903</td>\n",
       "      <td>0.203714</td>\n",
       "      <td>0.124273</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.844534</td>\n",
       "      <td>0.826475</td>\n",
       "      <td>1.361383e-01</td>\n",
       "      <td>0.072860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.912100</td>\n",
       "      <td>0.960854</td>\n",
       "      <td>0.202610</td>\n",
       "      <td>0.082109</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759804</td>\n",
       "      <td>0.762482</td>\n",
       "      <td>5.758946e-04</td>\n",
       "      <td>0.500824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.912019</td>\n",
       "      <td>0.960881</td>\n",
       "      <td>0.201660</td>\n",
       "      <td>0.148901</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.940639</td>\n",
       "      <td>0.672563</td>\n",
       "      <td>4.182561e-03</td>\n",
       "      <td>0.255453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.911994</td>\n",
       "      <td>0.960929</td>\n",
       "      <td>0.204755</td>\n",
       "      <td>0.093496</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.706706</td>\n",
       "      <td>0.734326</td>\n",
       "      <td>5.182465e-01</td>\n",
       "      <td>0.569428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>0.911701</td>\n",
       "      <td>0.960750</td>\n",
       "      <td>0.204188</td>\n",
       "      <td>0.126309</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.742425</td>\n",
       "      <td>0.821419</td>\n",
       "      <td>4.883618e-08</td>\n",
       "      <td>0.015254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0.911556</td>\n",
       "      <td>0.960639</td>\n",
       "      <td>0.203269</td>\n",
       "      <td>0.177046</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.832793</td>\n",
       "      <td>0.676556</td>\n",
       "      <td>5.474506e-02</td>\n",
       "      <td>0.724493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.910990</td>\n",
       "      <td>0.960219</td>\n",
       "      <td>0.203602</td>\n",
       "      <td>0.193880</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.861090</td>\n",
       "      <td>0.950996</td>\n",
       "      <td>2.745679e-08</td>\n",
       "      <td>0.025091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0.910893</td>\n",
       "      <td>0.960345</td>\n",
       "      <td>0.202722</td>\n",
       "      <td>0.120075</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.684128</td>\n",
       "      <td>0.728417</td>\n",
       "      <td>2.390631e-01</td>\n",
       "      <td>0.142969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trial     cv_ap    cv_auc  cv_logloss  learning_rate  max_depth  \\\n",
       "0      0  0.912578  0.961354    0.204983       0.087574          5   \n",
       "1      9  0.912544  0.961229    0.203870       0.085377          6   \n",
       "2      6  0.912128  0.960903    0.203714       0.124273          6   \n",
       "3      7  0.912100  0.960854    0.202610       0.082109          7   \n",
       "4      4  0.912019  0.960881    0.201660       0.148901          8   \n",
       "5      3  0.911994  0.960929    0.204755       0.093496          6   \n",
       "6      5  0.911701  0.960750    0.204188       0.126309          7   \n",
       "7      2  0.911556  0.960639    0.203269       0.177046          8   \n",
       "8      8  0.910990  0.960219    0.203602       0.193880          8   \n",
       "9      1  0.910893  0.960345    0.202722       0.120075          9   \n",
       "\n",
       "   min_child_weight  subsample  colsample_bytree     reg_alpha  reg_lambda  \n",
       "0                 3   0.880514          0.681704  9.572006e-06    0.094963  \n",
       "1                 8   0.815019          0.778330  5.983642e-02    0.069709  \n",
       "2                 4   0.844534          0.826475  1.361383e-01    0.072860  \n",
       "3                 3   0.759804          0.762482  5.758946e-04    0.500824  \n",
       "4                 5   0.940639          0.672563  4.182561e-03    0.255453  \n",
       "5                 5   0.706706          0.734326  5.182465e-01    0.569428  \n",
       "6                 7   0.742425          0.821419  4.883618e-08    0.015254  \n",
       "7                 8   0.832793          0.676556  5.474506e-02    0.724493  \n",
       "8                 8   0.861090          0.950996  2.745679e-08    0.025091  \n",
       "9                 7   0.684128          0.728417  2.390631e-01    0.142969  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"models/tuned_all_features_bo_tune_trials.csv\").sort_values(\"cv_ap\", ascending= False).head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airline-delay-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
